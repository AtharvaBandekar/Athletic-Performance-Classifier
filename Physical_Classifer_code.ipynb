{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "675aec5c-57b5-49af-abfa-3febd0a66356",
   "metadata": {},
   "source": [
    "# Workflow for Applying and Comparing Classifiers\r\n",
    "\r\n",
    "The code below demonstrates a practical workflow for:\r\n",
    "1. Applying and comparing multiple classifiers.\r\n",
    "2. Analyzing feature importance.\r\n",
    "3. Using the best-performing model to classify unlabeled data.\r\n",
    "\r\n",
    "## Classifiers to Implement\r\n",
    "The following classifiers will be tested:\r\n",
    "1. Logistic Regression\r\n",
    "2. Support Vector Machine (SVM)\r\n",
    "3. Random Forest\r\n",
    "4. Gradient Boosting\r\n",
    "5. Multilayer Perceptron (MLP)\r\n",
    "\r\n",
    "## Steps to Implement\r\n",
    "- Preprocess the data (handle missing values, scale features, encode labels).\r\n",
    "- Train and evaluate each classifier.\r\n",
    "- Use visualizations to show performance changes with increasing complexity.\r\n",
    "- Analyze feature importance using Ridge/Lasso regression.\r\n",
    "- Choose the most suitable classifier for the task and justify the choice.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4992d-55c3-4ac8-9cc6-c281ccf697d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules and packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad791f4d-cdbb-498f-bdca-93c0832e1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the dataset\n",
    "tr_performance = pd.read_csv(\"data-performance.csv\")\n",
    "\n",
    "#Right off the bat weight (and age?) seems to be the defining factor\n",
    "\n",
    "var_names = tr_performance.columns.to_numpy()[0:11]\n",
    "tr_performance['gender'].replace('F', '0.0', inplace = True)\n",
    "tr_performance['gender'].replace('M', '1.0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a905f9-0c1a-450b-bf76-d36019cc8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train models and evaluate performance (Optimized parameters below have been fed into this cell)\n",
    "\n",
    "# Split data\n",
    "X = tr_performance.iloc[:,0:11].to_numpy()\n",
    "y = tr_performance.iloc[:,11].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y, random_state=42)\n",
    "\n",
    "#Scale\n",
    "scalerX_train = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_s = scalerX_train.transform(X_train)\n",
    "\n",
    "scalerX_test = preprocessing.StandardScaler().fit(X_test)\n",
    "X_test_s = scalerX_test.transform(X_test)\n",
    "\n",
    "#Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=100000, C=100).fit(X_train_s, y_train)\n",
    "print(\"Logreg Training set score: {:.2f}\".format(logreg.score(X_train_s, y_train)))\n",
    "print(\" Logreg Test set score: {:.2f}\".format(logreg.score(X_test_s, y_test)))\n",
    "\n",
    "lg_pred = logreg.predict(X_test_s)\n",
    "print(classification_report(y_test, lg_pred))\n",
    "\n",
    "#Random Forest\n",
    "forest = RandomForestClassifier(n_estimators=500, random_state=0, max_depth = 15, max_features= None, min_samples_leaf =  1, min_samples_split = 10)\n",
    "forest.fit(X_train_s, y_train)\n",
    "print(\" RF Accuracy on training set: {:.2f}\".format(forest.score(X_train_s, y_train)))\n",
    "print(\" RF Accuracy on test set: {:.2f}\".format(forest.score(X_test_s, y_test)))\n",
    "\n",
    "rf_pred = forest.predict(X_test_s)\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "\n",
    "# #Gradient Boosting\n",
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate = 1)\n",
    "gbrt.fit(X_train_s, y_train)\n",
    "print(\"GB Accuracy on training set: {:.2f}\".format(gbrt.score(X_train_s, y_train)))\n",
    "print(\" GB Accuracy on test set: {:.2f}\".format(gbrt.score(X_test_s, y_test)))\n",
    "gb_pred = gbrt.predict(X_test_s)\n",
    "print(classification_report(y_test, gb_pred))\n",
    "\n",
    "\n",
    "# #svc\n",
    "svc = SVC(kernel='rbf', C=10, gamma = 'auto', degree = 2)\n",
    "svc.fit(X_train_s, y_train)\n",
    "print(\"SVC Accuracy on training set: {:.2f}\".format(svc.score(X_train_s, y_train)))\n",
    "print(\"SVC Accuracy on test set: {:.2f}\".format(svc.score(X_test_s, y_test)))\n",
    "svc_pred = svc.predict(X_test_s)\n",
    "print(classification_report(y_test, svc_pred))\n",
    "\n",
    "\n",
    "# #Multi-layer Perceptron\n",
    "mlp = MLPClassifier(random_state=42, max_iter=1000, hidden_layer_sizes=[50, 50], alpha = 0.01, learning_rate_init = 0.001)\n",
    "mlp.fit(X_train_s, y_train)\n",
    "print(\"MLP Accuracy on training set: {:.2f}\".format(mlp.score(X_train_s, y_train)))\n",
    "print(\"MLP Accuracy on test set: {:.2f}\".format(mlp.score(X_test_s, y_test)))\n",
    "mlp_pred = mlp.predict(X_test_s)\n",
    "print(classification_report(y_test, mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75fda3-56f0-4240-ae2e-436427fc300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation\n",
    "scores_lg = cross_val_score(logreg, X, y, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores_lg))\n",
    "\n",
    "scores_rf = cross_val_score(forest, X, y, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores_rf))\n",
    "\n",
    "scores_gb = cross_val_score(gbrt, X, y, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores_gb))\n",
    "\n",
    "scores_svc = cross_val_score(svc, X, y, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores_svc))\n",
    "\n",
    "scores_mlp = cross_val_score(mlp, X, y, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores_mlp))\n",
    "\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores_lg.mean()))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores_rf.mean()))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores_gb.mean()))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores_svc.mean()))\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores_mlp.mean()))\n",
    "\n",
    "res_lg = cross_validate(logreg, X, y, cv=5, return_train_score=True)\n",
    "res_rf = cross_validate(forest, X, y, cv=5, return_train_score=True)\n",
    "res_gb = cross_validate(gbrt, X, y, cv=5, return_train_score=True)\n",
    "res_svc = cross_validate(svc, X, y, cv=5, return_train_score=True)\n",
    "res_mlp = cross_validate(mlp, X, y, cv=5, return_train_score=True)\n",
    "\n",
    "res_lg_df = pd.DataFrame(res_lg)\n",
    "res_rf_df = pd.DataFrame(res_rf)\n",
    "res_gb_df = pd.DataFrame(res_gb)\n",
    "res_svc_df = pd.DataFrame(res_svc)\n",
    "res_mlp_df = pd.DataFrame(res_mlp)\n",
    "\n",
    "display(res_lg_df)\n",
    "display(res_rf_df)\n",
    "display(res_gb_df)\n",
    "display(res_svc_df)\n",
    "display(res_mlp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb075e-32bd-434f-8707-2caac84bc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Hyperparameter optimization\n",
    "# #Code has been commented as it will take hours to run the GridSearch. It can be un-commented and run if required. \n",
    "\n",
    "# #Logistic Regression\n",
    "# param_grid_lg = {\n",
    "#     'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "# }\n",
    "\n",
    "# grid_search_lg = GridSearchCV(LogisticRegression(random_state=42, max_iter=10000), param_grid_lg, cv=5)\n",
    "# grid_search_lg.fit(X_train_s, y_train)\n",
    "# print(\"Best parameters:\", grid_search_lg.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search_lg.best_score_)\n",
    "\n",
    "# #Gradient Boosting\n",
    "# param_grid_gb = {\n",
    "#     'learning_rate': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "#     'n_estimators': [100, 500, 1000],\n",
    "#      'max_depth': [1,3, 5, 7],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 5],\n",
    "#      'max_features': ['sqrt', 'log2', None]\n",
    "# }\n",
    "\n",
    "# grid_search_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb, cv=5)\n",
    "# grid_search_gb.fit(X_train_s, y_train)\n",
    "# print(\"Best parameters:\", grid_search_gb.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search_gb.best_score_)\n",
    "\n",
    "# #SVM\n",
    "# param_grid_svm = {\n",
    "#     'C': [0.1, 1, 10, 100],  \n",
    "#     'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  \n",
    "#     'gamma': ['scale', 'auto'],  \n",
    "#     'degree': [2, 3, 4]  \n",
    "# }\n",
    "\n",
    "# grid_search_svm = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=5)\n",
    "# grid_search_svm.fit(X_train_s, y_train)\n",
    "# print(\"Best parameters:\", grid_search_svm.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search_svm.best_score_)\n",
    "\n",
    "\n",
    "# #MLP\n",
    "# param_grid_mlp = {\n",
    "#     'hidden_layer_sizes': [(50,50), (80, 80), (100, 100), (50,50,50), (80, 80,80), (100, 100,100)],\n",
    "#     'alpha': [0.0001, 0.001, 0.01],\n",
    "#     'learning_rate_init': [0.001, 0.01],\n",
    "# }\n",
    "\n",
    "# grid_search_mlp = GridSearchCV(MLPClassifier(random_state=42, max_iter=1000), param_grid_mlp, cv=5)\n",
    "# grid_search_mlp.fit(X_train_s, y_train)\n",
    "# print(\"Best parameters:\", grid_search_mlp.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search_mlp.best_score_)\n",
    "\n",
    "# #Random Forest\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 500, 1000],\n",
    "#     'max_depth': [5, 10, 15],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 5],\n",
    "#     'max_features': ['sqrt', 'log2', None]\n",
    "# }\n",
    "\n",
    "# grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=0), param_grid_rf, cv=5)\n",
    "# grid_search_rf.fit(X_train_s, y_train)\n",
    "# print(\"Best parameters:\", grid_search_rf.best_params_)\n",
    "# print(\"Best cross-validation score:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25807129-16c2-4216-894d-cbf50d85e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "\n",
    "def plot_feature_importances_rf(model):\n",
    "    n_features = X_train_s.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), var_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "plot_feature_importances_rf(forest)\n",
    "print(forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06788e-28ad-416e-ac1f-51cb6cb8d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "\n",
    "new_performance = tr_performance.drop(['age'], axis=1)\n",
    "print(new_performance.head())\n",
    "\n",
    "\n",
    "X_new = new_performance.iloc[:,0:10].to_numpy()\n",
    "y_new = new_performance.iloc[:,10].to_numpy()\n",
    "\n",
    "print(X_new.shape)\n",
    "print(y_new)\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size = 0.2, stratify=y_new, random_state=42)\n",
    "\n",
    "scalerX_train_new = preprocessing.StandardScaler().fit(X_train_new)\n",
    "X_train_s_new = scalerX_train_new.transform(X_train_new)\n",
    "\n",
    "scalerX_test_new = preprocessing.StandardScaler().fit(X_test_new)\n",
    "X_test_s_new = scalerX_test_new.transform(X_test_new)\n",
    "\n",
    "forest_new = RandomForestClassifier(n_estimators=500, random_state=0, max_depth = 15, max_features= None, min_samples_leaf =  1, min_samples_split = 10)\n",
    "forest_new.fit(X_train_s_new, y_train_new)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest_new.score(X_train_s_new, y_train_new)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest_new.score(X_test_s_new, y_test_new)))\n",
    "\n",
    "#Dropping different features made no improvements to test scores, it lowered them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425f5fc-a885-4388-80cf-8b9fdf3e47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphs and Charts for the report\n",
    "\n",
    "x_bar = tr_performance.iloc[:,11]\n",
    "df_classes = pd.DataFrame({'Class': x_bar})\n",
    "custom_order = ['A', 'B', 'C', 'D'] \n",
    "sb.countplot(data=df_classes, x='Class', palette = 'PuBuGn', order=custom_order)\n",
    "plt.title(\"Number of Individuals in Each Class\", fontsize=12)\n",
    "plt.xlabel(\"Class\", fontsize=10)\n",
    "plt.ylabel(\"Number of Individuals\", fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55b716-d74d-4499-99cf-d2740b2ef9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_classes['Class'].value_counts(normalize=True) * 100 \n",
    "cp1 = pd.DataFrame(class_counts.index, columns=['Class'])\n",
    "cp2 = pd.DataFrame(class_counts.values, columns=['Percentages'])\n",
    "class_percentages = pd.merge(cp1, cp2, left_index=True, right_index=True)\n",
    "class_percentages.head()\n",
    "sb.barplot(data=class_percentages, x='Class', y='Percentages', palette='PuBuGn', order=custom_order)\n",
    "plt.title(\"Percentage of Individuals in Each Class\", fontsize=12)\n",
    "plt.xlabel(\"Class\", fontsize=10)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ea921-1e44-4584-8f35-db8239af140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(tr_performance, hue=\"class\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1479fce-1796-4950-b372-5a5199c64a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA analysis visualization\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data-performance.csv')\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Encode 'gender' and 'class' columns\n",
    "le_gender = LabelEncoder()\n",
    "le_class = LabelEncoder()\n",
    "data['gender'] = le_gender.fit_transform(data['gender'])  # Encode gender\n",
    "data['class_encoded'] = le_class.fit_transform(data['class'])  # Encode class\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['class', 'class_encoded'], axis=1)\n",
    "y = data['class_encoded']\n",
    "\n",
    "print(y.head())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, ticks=range(len(le_class.classes_)), label='Class')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('PCA Visualization of Class Clusters')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b004d2-9a05-4892-91db-e2e9c3a46373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Converts ['B', 'C', 'D'] to numeric labels like [0, 1, 2]\n",
    "\n",
    "#Perform k-Means Clustering\n",
    "n_clusters = len(np.unique(y_train_encoded))  # Set the number of clusters to the number of unique classes\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_train_s)\n",
    "\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
    "X_tsne = tsne.fit_transform(X_train_s)\n",
    "\n",
    "#Plot t-SNE visualization with k-Means clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_labels, cmap='viridis', s=20)\n",
    "plt.colorbar(scatter, ticks=range(len(le_class.classes_)), label='k-Means Cluster')\n",
    "plt.title(\"t-SNE Visualization of k-Means Clusters\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()\n",
    "\n",
    "#Compare k-Means Clusters with True Labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_train_encoded, cmap='viridis', s=20)\n",
    "plt.colorbar(scatter, ticks=range(len(le_class.classes_)),label='True Labels')\n",
    "plt.title(\"t-SNE Visualization of True Labels\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc709e6c-5933-403e-8f31-288b7082d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on Labelled data\n",
    "\n",
    "unlabelled_data = pd.read_csv('data-performance-test-unlabelled.csv')\n",
    "\n",
    "\n",
    "var_names = unlabelled_data.columns.to_numpy()[0:11]\n",
    "unlabelled_data['gender'].replace('F', '0.0', inplace = True)\n",
    "unlabelled_data['gender'].replace('M', '1.0', inplace = True)\n",
    "\n",
    "X_unlabelled = unlabelled_data\n",
    "X_unlabelled_scaled = scaler.transform(X_unlabelled)\n",
    "\n",
    "predictions = forest.predict(X_unlabelled_scaled)\n",
    "\n",
    "\n",
    "unlabelled_data['Predicted Class'] = predictions\n",
    "\n",
    "\n",
    "print(unlabelled_data.head())\n",
    "\n",
    "unlabelled_data.to_csv('predicted_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da842f70-9fe1-4cc8-819a-648bb4430656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for report\n",
    "x_bar_pred = unlabelled_data.iloc[:,11]\n",
    "df_classes_pred = pd.DataFrame({'Class': x_bar_pred})\n",
    "custom_order = ['A', 'B', 'C', 'D'] \n",
    "sb.countplot(data=df_classes_pred, x='Class', palette = 'PuBuGn', order=custom_order)\n",
    "plt.title(\"Number of Individuals in Each Class\", fontsize=12)\n",
    "plt.xlabel(\"Class\", fontsize=10)\n",
    "plt.ylabel(\"Number of Individuals\", fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a59819-3784-4dd6-9308-f110a2b1196e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
